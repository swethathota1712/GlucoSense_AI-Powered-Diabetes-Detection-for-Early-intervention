# Step 1: Handle Missing Values (HbA1c_level)
df["HbA1c_level"] = df["HbA1c_level"].replace({"???": np.nan, "Unknown": np.nan})
df["HbA1c_level"] = df.groupby("diabetes")["HbA1c_level"].transform(lambda x: x.fillna(x.median()))

# Step 2: Encode Categorical Features
# One-Hot Encoding for Gender and Smoking History
# Check if 'gender' and 'smoking_history' are already one-hot encoded
# If they are, skip this step
if 'gender' in df.columns and 'smoking_history' in df.columns:
    ohe = OneHotEncoder(drop="first", sparse_output=False, handle_unknown='ignore') # sparse=False to get array, handle_unknown to avoid errors with unseen values
    encoded_features = ohe.fit_transform(df[["gender", "smoking_history"]])
    encoded_df = pd.DataFrame(encoded_features, columns=ohe.get_feature_names_out(["gender", "smoking_history"]))
    df = df.drop(columns=["gender", "smoking_history"]).join(encoded_df)
else:
    print("Gender and/or Smoking History columns are already one-hot encoded or not present.")

# ... rest of your code ...
# Step 3: Scale Numerical Features
scaler_standard = StandardScaler()
df[["blood_glucose_level", "HbA1c_level"]] = scaler_standard.fit_transform(df[["blood_glucose_level", "HbA1c_level"]])

scaler_minmax = MinMaxScaler()
df[["bmi", "age"]] = scaler_minmax.fit_transform(df[["bmi", "age"]])

# Step 4: Handle Outliers using IQR
def remove_outliers_iqr(data, col):
    Q1 = data[col].quantile(0.25)
    Q3 = data[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[col] >= lower_bound) & (data[col] <= upper_bound)]

outlier_cols = ["blood_glucose_level", "bmi", "age"]
for col in outlier_cols:
    df = remove_outliers_iqr(df, col)

# Step 5: Train-Test Split
X = df.drop(columns=["diabetes"])  # Features
y = df["diabetes"]  # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 6: Handle Class Imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train, y_train)

# Step 7: Feature Selection using Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
feature_importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)

# Select top important features
selected_features = feature_importances[feature_importances > 0.01].index
X_train = X_train[selected_features]
X_test = X_test[selected_features]

# Step 8: Build and Evaluate a Logistic Regression Model
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
